---
title: "Models simulation - summary"
subtitle: "v1, XIFF v0.0.17, 2021-07-09"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

# Simulation setting

For each model:

  - Random Forest (rf)
  - SVM (svmLinear2)
  - Neural Network (neuralnetwork)
  - Regularized Logistic Regression (glmnet)
  
two feature selection algorithms were tested:

  - Boruta - using two thresholds:
    - Confirmed - more conservative, selects only the features that was able to confirm.
    - Tentative - selects even the features which thier importance is uncertain.
  - T-test filter with threshold = `0.05`
    - implementation based on `genefilter::colttests`.
    
and 3 thresholds for number of features:

  - 5
  - 50
  - unlimited (any number that came from feature selection algorithm).
  
This lead to `4 (models) x 3 (feature selection methods) x 3 (feature thresholds) = 36 combinations`.

Each simulation was conducted for 5 data sets:

  - Programmer's import: 0766b1 using gene sets P53_PATHWAY or APOPTOSIS
  - Programmer's import: 81c5cc using gene set EPITHELIAL_MESENCHYMAL_TRANSITION, E2F_TARGETS and G2M_CHECKPOINT.

giving `36 combinations x 5 = 180` different combinations.

Each model was trained 50 times with different random.seed.

# Results

```{r}
simulationResult <- readRDS("simulation-result-v2-2021-07-16.rds")

makeSummaryTable <- function(tbl, stat) {

  stat <- enquo(stat)
  tbl2 <- tbl %>%
    group_by(Models, FeatureAlgo, FeatureNumbers, hash, hallmarkGeneSet) %>%
    summarise(
      MEAN = mean(!!stat),
      Q05  = quantile(!!stat, 0.05),
      Q95  = quantile(!!stat, 0.05),
      MIN  = min(!!stat),
      MAX  = max(!!stat),
      IQR = IQR(!!stat)
    ) %>% ungroup()
    
  tbl2
}


MODEL_NAMES <- simulationResult$Models %>% unique()
MODEL_COLORS = setNames(
  RColorBrewer::brewer.pal(length(MODEL_NAMES), name = "Dark2"),
  MODEL_NAMES)

library(ggplot2)
makePlot <- function(tbl, stat) {
  
  stat = enquo(stat)
  
  tbl <- tbl %>% mutate(
    FeatureNumbers = as.numeric(as.factor(FeatureNumbers))
  )
  
  p <- ggplot(tbl) + 
  geom_line(aes(FeatureNumbers, !!stat, color = Models, linetype = FeatureAlgo)) + 
  facet_wrap(c("hash", "hallmarkGeneSet"))

  p <- p +
    theme_bw() +
    scale_x_continuous(breaks = c(1,2,3), labels = c("5", "50", "Inf")) + 
    scale_color_manual(values = MODEL_COLORS) + 
    scale_y_continuous(labels = scales::percent)
  
  
  p
}

makeBoxPlot <- function(tbl, stat) {
  
  stat = enquo(stat)
  
  tbl <- tbl %>% mutate(
    FeatureNumbers = as.numeric(as.factor(FeatureNumbers))
  )
  
  tbl <- tbl %>% mutate(Model = paste(Models, FeatureAlgo, sep = " - "))
  
  p <- ggplot(tbl) + 
  geom_boxplot(aes(Model, !!stat)) + 
  facet_wrap(c("hash", "hallmarkGeneSet", "FeatureNumbers")) + 
    coord_flip()

  p <- p +
    theme_bw() +
    scale_x_continuous(breaks = c(1,2,3), labels = c("5", "50", "Inf")) + 
    scale_color_manual(values = MODEL_COLORS) + 
    scale_y_continuous(labels = scales::percent)
  
  
  p
}


accuracyTable <- makeSummaryTable(simulationResult, Accuracy)

```

## Accuracy

All models:

```{r, fig.width=12, fig.height=10}
makePlot(accuracyTable, stat = MEAN) + ggtitle("Accuracy - Mean")
makePlot(accuracyTable, stat = Q05) + ggtitle("Accuracy - Q05")
makePlot(accuracyTable, stat = IQR) + ggtitle("Accuracy - IQR")
```

Observation:

- Neural Network seems to be the worst model in all scenarios. There must be some problem with preprocessing or data scaling. Some difference between models is expected, but not that big.
- The good news is that it is clearly visible that adding more features to Neural Network decrease the performance.
  - For future simulations there's no need to use more than 50 features (we can run the more granular simulation in the low range e.g. for 5, 10, 15 features).
- There's nearly no difference between models in `81c5cc x (E2F_TARGETS or G2M_CHECKPOINT)` scenario.

Let's zoom a little bit into data without Neural Network and `81c5cc x (E2F_TARGETS or G2M_CHECKPOINT)`:

```{r, fig.width=14, fig.height=6}
filteredData <- accuracyTable %>%
  filter(
    Models != "neuralnetwork",
     !(
       hash  == "81c5cc" & 
         hallmarkGeneSet %in% c("E2F_TARGETS", "G2M_CHECKPOINT")
      )
  )

makePlot(filteredData, stat = MEAN) + ggtitle("Accuracy - Mean")
makePlot(filteredData, stat = Q05) + ggtitle("Accuracy - Q05")
makePlot(filteredData, stat = IQR) + ggtitle("Accuracy - IQR")
```
Observations:

- GLMNET has good performance in each scenario, however the differences between models are not that big (1%-2% point).
- More features seems to (on average) imporves the model, which makes sense from  the thoeretical stand point:
 - GLMNET has an implicit feature selection method, which takes care of getting the most information from the data in the given context. For that model using any feature selection algorithm might help to speed up the computation time by removing *obvious* non-required features.
 - In our case, using the `fastest` method might be a good idea.
 
# Conclusions:

- There's some issue with the current NN implementation - it's unstable and gives poor results (I'll loook into that, probably some data preprocessing is required, glmnet does its own preprocessing and it seems to work well).
- Feature selection method seems to not provide that much difference. However, the `t-test` prefers variables that are linearly separable and in some cases might not be optimal. See the example below.
- Number of features matter - most algos prefer more features.
  - My guess is that there's no visible difference between the `50` and `Inf` thresholds for number of features, because in most cases the feature selection algorithm returns `~50` variables.
  

Example of non-linear relation:
  
```{r}
set.seed(123)

n <- 50
x <- c(rnorm(n, mean = 0), rnorm(n, mean = 10))
y <- rnorm(2*n, mean = 5, sd = sd(x))

# T-test shows no-difference
t.test(x,y)

xy <- tibble(
  class = c(rep("x", 2*n), rep("y", 2*n)),
  value = c(x,y)
)

#  Class y dominates in the middle of the plot 
ggplot(xy) + geom_density(aes(value, color = class)) + theme_bw()

if(require("FSelectorRcpp")) {
  
  # Simple discretization algorithm is able to catch
  # the non-linearlity in the data
  xy <- xy %>% mutate(class = factor(class), value = round(value, 3))
  
  ggplot(FSelectorRcpp::discretize(class ~ value, xy)) +
    geom_bar(aes(value, fill = class))
  
}

```
  
  
