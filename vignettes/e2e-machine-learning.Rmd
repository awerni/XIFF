---
title: "E2E Machine Learning using XIFF"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{E2E Machine Learning using XIFF}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "CairoPNG",
  fig.height = 6,
  fig.width = 7,
  eval = FALSE
)
```

*The code in this vignette is not evaluated because it takes too much time to run. It is here only as a reference.*

# Preparing the data

```{r setup}
library(XIFF)
library(dplyr)
setDbOptions()
set.seed(123)

# Preparing data
cellline <- CLIFF::getWaterfallDataDepletion(ensg = "ENSG00000135679", study = "Avana", score = "ceres")
cellline <- cellline %>% filter(tumortype %in% c("skin cancer", "lung cancer",  "ovarian cancer", "pancreatic cancer"))
cellline <- cellline %>% filter(var < 0)

# Select sensitive and resistant celllines
# note that the number of resistant celllines is selected to be the same as number of sensistve celllines 
sensitive <- cellline %>% filter(var < -1) %>% select(celllinename) %>% pull %>% as.character()
resistant <- cellline %>% arrange(desc(var)) %>% head(length(sensitive)) %>% select(celllinename) %>% pull %>% as.character()
cs <- list(class1 = sensitive, class2 = resistant)
```

```{r}
cs <- getStashedData("0766b1")

cs <- list(
      class1 = cs %>% filter(MDM2_class == "resistant") %>% pull,
      class2 = cs %>% filter(MDM2_class == "sensitive") %>% pull
    )

```

```{r}

sets <- XIFF::splitTrainingValidationSets(XIFF::stackClasses(cs, return_factor = TRUE), 0.2)
sets$training

trainingSet <- with(sets$training, split(celllinename, class))
validationSet <- with(sets$validation, split(celllinename, class))

# Hallmark gene set
hallmarkGeneSet <- "P53_PATHWAY"
geneSet <- CLIFF::getGSEAdata("human", "hallmark")[[paste0("HALLMARK_", hallmarkGeneSet)]]

# Gene Annotation
geneAnno <- CLIFF::getGeneAnno()[["human"]]
```

# Training the model using `buildMachineLearning`

```{r}
set.seed(123)
defaultFit <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
)

nnFit <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "neuralnetwork"
)

# model using caret functionality. It does not implement 
# model specific function is XIFF, but uses sensible defaults
glmnetFit <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "glmnet"
)
```
# Making Predictions

### Prediction just uses `S3 predict` method from `caret`.

```{r}
predict(defaultFit)
predict(nnFit)
predict(glmnetFit)
```

### Prediction on model's validation set (note that each model has it's own validation created before training):

```{r}
# Random forest
## Getting data
modelValidationData <- getDataForModel(assignment = defaultFit$validationSet,
                                       features = defaultFit)
## Making predition
predict(defaultFit, newdata = modelValidationData)

# GLMNET
## Getting data
modelValidationDataGlmnet <- getDataForModel(assignment = glmnetFit$validationSet,
                                       features = glmnetFit)
## Making predition
predict(glmnetFit, newdata = modelValidationDataGlmnet)
```
### Using the validation set created at the beginning.

```{r}
# Getting all features from rf and glmnet models
# so the same data can be used in prediction
allFeatures <- list(defaultFit, glmnetFit) %>% 
  sapply("[[", "bestFeatures") %>%
  unlist() %>% unique()

modelValidationData <- getDataForModel(assignment = validationSet,
                                       features = allFeatures)

# Making the prediction for two models
prf <- predict(defaultFit, newdata = modelValidationData)
pglm <- predict(glmnetFit, newdata = modelValidationData)

# Adding the predictions to the table with celllinename and class
modelValidationData %>% dplyr::select(celllinename, class) %>%
  mutate(RandomForest = prf, Glmnet = pglm)

```

# Plots

## Basic Performance Plots

```{r}
library(tidyr)
generatePerformancePlot(defaultFit)
generatePerformancePlot(nnFit)
generatePerformancePlot(glmnetFit)
```
## Variable importance

```{r}
generateVarImpPlot(defaultFit)
generateVarImpPlot(nnFit)
generateVarImpPlot(glmnetFit)
```

## Error plot (random forest only)

```{r}
# For random forest only
generateErrorPlot(defaultFit)
```



```{r}
validationData <- getDataForModel(assignment = validationSet, features = defaultFit)
annoFocus <- CLIFF::getCellLineAnno("human") %>% dplyr::filter(celllinename %in% unlist(validationSet))


predictionSummary <- getPredictionSummary(
  items = validationData$celllinename,
  preds = predict(defaultFit, newdata = validationData),
  refs = validationData$class,
  positive_model = "class1",
  positive_cs = "class1",
  classes = c("positive", "negative"),
  classes_model = c("sensitive", "resistant"),
  classes_cs = c("sensitive", "resistant"),
  annoFocus = annoFocus
)

predictionSummary$res

predictionSummary$data
```


```{r}
df <- prepareTablePlotData(
  df = predictionSummary$data,
  positive_preds = "class1",
  positive_refs = "class1",
  labels_preds = c("sensitive", "resistant"),
  labels_refs = c("sensitive", "resistant"),
  labels = c("positive", "negative")
)
generateTablePlot(df)

df2 <- getPerformanceDataFrame(predictionSummary$res$table)
generateApplyPerformancePlot(df2)


gatherPredictionResults(predictions = list(predictionSummary))


validation <- validateModel(
  defaultFit,
  validationSet = stackClasses(validationSet),
  anno = annoFocus)
```


```{r}
makeModelPlots <- function(model, validationSet) {
  
  annoFocus <- CLIFF::getCellLineAnno("human") %>% dplyr::filter(celllinename %in% unlist(validationSet))
  
  validation <- validateModel(
    model,
    validationSet = stackClasses(validationSet),
    anno = annoFocus
  )
  
  df <- prepareTablePlotData(
    df = validation$data,
    positive_preds = "class1",
    positive_refs = "class1",
    labels_preds = c("sensitive", "resistant"),
    labels_refs = c("sensitive", "resistant"),
    labels = c("positive", "negative")
  )
  
  
  
  
  df2 <- getPerformanceDataFrame(validation$res$table)
  list(
    TablePlot <- generateTablePlot(df),
    ApplyPerformancePlot = generateApplyPerformancePlot(df2),
    PerformancePlot = generatePerformancePlot(model),
    VariableImportancePlot = generateVarImpPlot(model)
  )
}

```


### Random Forest

```{r}
makeModelPlots(defaultFit, validationSet)
```

### Neural Network

```{r}
makeModelPlots(nnFit, validationSet)
```

### Glmnet

```{r}
makeModelPlots(glmnetFit, validationSet)
```

# More examples.

## SVM

```{r}
svmFit <- XIFF::buildMachineLearning(
  cs = cs,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "svmLinear2"
)

makeModelPlots(svmFit, validationSet)
```

## CART

```{r}

if(XIFF::packageInstalled("rpart")) {
  rpartFit <- XIFF::buildMachineLearning(
    cs = cs,
    geneSet = geneSet,
    geneAnno = geneAnno,
    method = "rpart"
  )
  makeModelPlots(rpartFit, validationSet)
}

```


# Custom feature selection function

```{r}
fitAffinity <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "glmnet",
  selectBestFeaturesFnc = XIFF::selectBestFeaturesAffinityPrefilter,
  threshold = 0.05
)

fitAffinity$featureSelectionResult$method
fitAffinity$bestFeatures

fitAffinityPostfilter <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "glmnet",
  selectBestFeaturesFnc = XIFF::selectBestFeaturesAffinityPostfilter,
  threshold = 0.05
)
fitAffinityPostfilter$featureSelectionResult$method
fitAffinityPostfilter$bestFeatures
```

```{r}
fitGlmnet <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "glmnet",
  selectBestFeaturesFnc = selectBestFeaturesGlmnet,
  threshold = "auto"
)

```


# Custom data function

```{r}

assignment <- trainingSet
features <- geneSet
getModelWithRatio <- function(assignment, features) {
  
  dt <- getDataForModel(assignment, features)
  
  prefilteredFeatures <- selectBestFeaturesTTest(dt %>%
                                        select(-celllinename) %>%
                                        mutate(class = as.factor(class)),
                                        maxFeatures = 20)
  prefilteredFeatures <- prefilteredFeatures$stats$ensg
  
  numDt <- colnames(dt  %>% select_at(prefilteredFeatures) %>% select_if(is.numeric))
  numDt <- sort(numDt)
  
  dts <- expand.grid(numDt, numDt) %>% filter(Var1 != Var2) %>%
    mutate_all(as.character) %>%
    filter(Var1 < Var2)
  
  matx <- dt %>% select_if(is.numeric) %>% as.matrix()
  matx[matx == 0] <- 0.0001
  ratio <- matx[,dts$Var1] / matx[,dts$Var1]
  colnames(ratio) <- paste(dts$Var1, dts$Var2, sep = "_")
  ratioDf <- as.data.frame(ratio)
  result <- bind_cols(dt, ratioDf)
  result
}

fitAffinityWithRatio <- XIFF::buildMachineLearning(
  cs = trainingSet,
  geneSet = geneSet,
  geneAnno = geneAnno,
  method = "glmnet",
  selectBestFeaturesFnc = XIFF::selectBestFeaturesAffinity,
  getDataForModelFnc = getModelWithRatio
)

fitAffinityWithRatio$featureSelectionResult$method
fitAffinityWithRatio$bestFeatures

```
